{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTACIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from datetime import timedelta\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from streamlit_echarts import st_echarts\n",
    "import streamlit as st\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validar_y_actualizar(fila):\n",
    "    if  fila[\"cantidad_unid\"] >= 1:\n",
    "        if (fila['id_item'] == 13887): #JERINGA MEGA INSUL 1MLx29Gx1/2x100\n",
    "          fila[\"cantidad_frac\"] += 100 * int(fila[\"cantidad_unid\"])\n",
    "          #datos = datos = datos.rename(columns={'cantidad_unid': 'cantidad_frac'})\n",
    "          #fila[\"cantidad_unid\"] = 0\n",
    "        elif fila['id_item'] in {90765, 79680, 27112, 1669, 101609,}: #x'unidad  \n",
    "          fila[\"cantidad_frac\"] += int(fila[\"cantidad_unid\"])\n",
    "\n",
    "        elif(fila['id_item'] == 54122): #XARELTO COM-RECx10MGx10\n",
    "          fila[\"cantidad_frac\"] += 10 * int(fila[\"cantidad_unid\"])\n",
    "        \n",
    "        elif(fila['id_item'] == 88275): #MICARDIX\n",
    "           fila['cantidad_frac'] += 28 * int(fila[\"cantidad_unid\"])  \n",
    "    return fila\n",
    "\n",
    "\n",
    "# Preparar los datos\n",
    "def preparar_datos(datos):\n",
    "    # Seleccionar columnas específicas\n",
    "    columnas_especificas = ['Fecha', 'id_item', 'cantidad_unid', 'cantidad_frac']\n",
    "    datos = datos[columnas_especificas]\n",
    "\n",
    "    # Aplicar la función a cada fila\n",
    "    datos = datos.apply(validar_y_actualizar, axis=1)\n",
    "    datos = datos.drop(columns=[\"cantidad_unid\"])\n",
    "\n",
    "    print(datos)\n",
    "    # Convertir la columna Fecha a formato datetime\n",
    "    datos['Fecha'] = pd.to_datetime(datos['Fecha'], format='%d/%m/%Y %H:%M')\n",
    "    # Establecer la hora y el minuto a 0\n",
    "    datos['Fecha'] = datos['Fecha'].apply(lambda dt: dt.replace(hour=0, minute=0, second=0))\n",
    "\n",
    "    # Agrupar por Fecha e id_item y sumar cantidad_frac\n",
    "    datos = datos.groupby(['Fecha', 'id_item'], as_index=False).sum()\n",
    "\n",
    "    # Ordenar el dataset por Fecha\n",
    "    datos.sort_index(inplace=False)\n",
    "    datos = datos.set_index('Fecha')\n",
    "\n",
    "    return datos\n",
    "\n",
    "# Definir la función de pérdida RMSE\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return tf.math.sqrt(tf.math.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "# Cargar el modelo y el escalador guardados\n",
    "def cargar_modelo_y_scaler(id_item):\n",
    "    modelo_path = f'../data/modelo_{id_item}.keras'\n",
    "    scaler_path = f'../data/scaler_{id_item}.pkl'\n",
    "\n",
    "    with open(scaler_path, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    modelo = load_model(modelo_path, custom_objects={'root_mean_squared_error': root_mean_squared_error})\n",
    "\n",
    "    return modelo, scaler\n",
    "\n",
    "# Predecir futuros valores\n",
    "def predecir(x, model, scaler):\n",
    "    y_pred_s = model.predict(x, verbose=0)\n",
    "    y_pred = scaler.inverse_transform(y_pred_s)\n",
    "    return y_pred.flatten()\n",
    "\n",
    "# Reinterpolar los datos para cada id_item sin cambiar el índice\n",
    "def reinterpolar_datos_por_id(datos):\n",
    "    datos_reinterpolados = pd.DataFrame()\n",
    "\n",
    "    for id_item in datos['id_item'].unique():\n",
    "        df_item = datos[datos['id_item'] == id_item].copy()\n",
    "\n",
    "        # Reinterpolar con frecuencia diaria\n",
    "        df_item = df_item.asfreq(freq='D', fill_value=0)\n",
    "\n",
    "        # Volver a agregar el id_item\n",
    "        df_item['id_item'] = id_item\n",
    "\n",
    "        # Concatenar los resultados\n",
    "        datos_reinterpolados = pd.concat([datos_reinterpolados, df_item])\n",
    "\n",
    "    return datos_reinterpolados.reset_index()\n",
    "\n",
    "def generar_predicciones_futuras(df, id_item, input_length, num_predicciones):\n",
    "    modelo, scaler = cargar_modelo_y_scaler(id_item)\n",
    "\n",
    "    ultima_fecha = df['Fecha'].iloc[-1] # Access the last date from the 'Fecha' column\n",
    "    print(ultima_fecha)\n",
    "    fechas_futuras = [ultima_fecha + timedelta(days=i) for i in range(1, num_predicciones + 1)]\n",
    "\n",
    "    ultimo_segmento = df['cantidad_frac'][-input_length:].values\n",
    "    ultimo_segmento = ultimo_segmento.reshape((1, input_length, 1))\n",
    "\n",
    "    predicciones_futuras = []\n",
    "    segmento_actual = ultimo_segmento\n",
    "\n",
    "    for _ in range(num_predicciones):\n",
    "        prediccion = predecir(segmento_actual, modelo, scaler)\n",
    "        predicciones_futuras.append(prediccion[0])\n",
    "\n",
    "        nuevo_valor = np.array(prediccion[0]).reshape(1, 1, 1)\n",
    "        segmento_actual = np.append(segmento_actual[:, 1:, :], nuevo_valor, axis=1)\n",
    "\n",
    "    # Crear un DataFrame con las predicciones futuras, fechas y el id_item correspondiente\n",
    "    resultados_futuros = pd.DataFrame({\n",
    "        'Fecha': fechas_futuras,\n",
    "        'Predicción': predicciones_futuras,\n",
    "        'id_item': id_item  # Agregar el id_item correspondiente\n",
    "    })\n",
    "\n",
    "    return resultados_futuros\n",
    "\n",
    "\n",
    "\n",
    "# Ejecutar todo el proceso para todos los id_item\n",
    "def predecir_para_todos_los_items(datos, input_length, num_predicciones):\n",
    "    resultados_totales = pd.DataFrame()\n",
    "    lista = [90765, 27112, 13887, 90765, 79680, 1669, 101609, 54122, 88275]\n",
    "    for id_item in lista:\n",
    "        # Filtrar los datos para el id_item actual\n",
    "        df_item = datos[datos['id_item'] == id_item]\n",
    "        # Generar predicciones para este id_item\n",
    "        resultados_item = generar_predicciones_futuras(df_item, id_item, input_length, num_predicciones)\n",
    "        # Concatenar los resultados al DataFrame total\n",
    "        resultados_totales = pd.concat([resultados_totales, resultados_item])\n",
    "\n",
    "    return resultados_totales\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    st.title('Predicción de Ventas')\n",
    "\n",
    "    uploaded_file = st.file_uploader(\"Sube tu archivo CSV\", type=\"csv\")\n",
    "    if uploaded_file is not None:\n",
    "        datos = pd.read_csv(uploaded_file, delimiter=';')\n",
    "        datos = preparar_datos(datos)\n",
    "        datos = reinterpolar_datos_por_id(datos)\n",
    "        resultados_futuros = predecir_para_todos_los_items(datos, 24, 4)\n",
    "        \n",
    "        st.write(\"Resultados de Predicción:\")\n",
    "        st.dataframe(resultados_futuros)\n",
    "        lista = [90765, 27112, 13887, 90765, 79680, 1669, 101609, 54122, 88275]\n",
    "\n",
    "        #Graficar ventas promedio\n",
    "        for id_item in lista:\n",
    "            plot_ventas_promedios(datos, id_item)\n",
    "\n",
    "        # Graficar para cada id_item\n",
    "        \n",
    "        for id_item in lista:\n",
    "            df_item = datos[datos['id_item'] == id_item]\n",
    "            predicciones_item = resultados_futuros[resultados_futuros['id_item'] == id_item]\n",
    "            st.write(f\"Gráfico para id_item {id_item}\")\n",
    "            plot_predicciones_vs_realidad(df_item, predicciones_item, id_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 23:11:00.316 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\jeanf\\miniconda3\\envs\\streamlit\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRÁFICOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ventas_promedios(datos, id_item):\n",
    "    datos[datos['id_item'] == id_item].inplace = True\n",
    "\n",
    "    axis = datos.groupby(datos['Fecha'].dt.month)[['cantidad_frac']].mean().plot(figsize = (10,5), marker = 'o', color='r')\n",
    "    axis.set_title(f'Ventas promedio por Mes {id_item}')\n",
    "\n",
    "    axis = datos.groupby(datos['Fecha'].dt.year)[['cantidad_frac']].mean().plot(figsize = (10,5), marker = 'o', color='r')\n",
    "    axis.set_title(f'Ventas promedio por Año {id_item}')\n",
    "\n",
    "    axis = datos.groupby(datos['Fecha'].dt.day)[['cantidad_frac']].mean().plot(figsize = (10,5), marker = 'o', color='r')\n",
    "    axis.set_title(f'Ventas promedio por Día {id_item}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicciones_vs_realidad(df, predicciones, id_item):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(df.index, df['cantidad_frac'], label='Cantidad Real')\n",
    "    plt.plot(predicciones['Fecha'], predicciones['Predicción'], label='Predicción', linestyle='--')\n",
    "    plt.title(f'Predicciones vs Realidad para id_item {id_item}')\n",
    "    plt.xlabel('Fecha')\n",
    "    plt.ylabel('Cantidad')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    st.pyplot(plt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "provincias_y_ciudades = {\n",
    "    'Guayas': ['crnel. marcelino maridueña', 'alfredo baquerizo moreno (jujan)', 'duran', 'gnral. antonio elizalde', 'lomas de sargentillo', 'playas', 'samborondon', 'santa lucia', 'simon bolivar', 'Guayaquil', 'Durán', 'Daule', 'Milagro', 'Samborondón', 'Velasco Ibarra', 'Naranjal', 'Balzar', 'El Triunfo', 'Yaguachi', 'Pedro Carbo', 'Salitre', 'Alfredo Baquerizo Moreno', 'Naranjito', 'San Miguel', 'Píllaro', 'Simón Bolívar', 'Santa Lucía', 'Balao', 'Palestina', 'Narcisa de Jesús', 'Colimes', 'El Empalme', 'Isidro Ayora', 'Nobol', 'San Jacinto de Yaguachi'],\n",
    "    'Pichincha': ['mejia', 'Quito', 'Sangolquí', 'Cayambe', 'Pedro Vicente Maldonado', 'Puerto Quito', 'Machachi', 'Tabacundo', 'Mejía', 'Pedro Moncayo', 'Rumiñahui', 'San Miguel de Los Bancos'],\n",
    "    'Azuay': ['camilo ponce enriquez', 'giron', 'Cuenca', 'Gualaceo', 'Paute', 'Chordeleg', 'Camilo Ponce Enríquez', 'Nabón', 'Guachapala', 'Pucará', 'San Fernando', 'Sevilla de Oro', 'El Pan', 'Girón', 'Oña', 'Las Lajas', 'Santa Isabel'],\n",
    "    'Sto. Domingo': ['santo domingo de los tsachilas', 'Santo Domingo', 'La Concordia'],\n",
    "    'El Oro': ['marcabeli', 'Machala', 'Pasaje', 'Santa Rosa', 'Huaquillas', 'Piñas', 'El Guabo', 'Arenillas', 'Balsas', 'Marcabelí', 'Portovelo', 'Zaruma', 'Las Naves', 'Atahualpa', 'Chilla'],\n",
    "    'Manabí': ['jaramijo', 'junin', 'pajan', 'puerto lopez', 'Manta', 'Portoviejo', 'Montecristi', 'Chone', 'El Carmen', 'Jipijapa', 'Rocafuerte', 'Santa Ana', 'San Vicente', 'Olmedo', 'Paján', 'Sucre', 'Bahía de Caráquez', 'Jaramijó', 'Pedernales', 'Puerto López', 'Jama', '24 de Mayo', 'Bolívar', 'Chinchipe', 'Flavio Alfaro', 'Junín', 'Pichincha', 'Paján', 'Puerto López', 'Tosagua'],\n",
    "    'Loja': ['espindola', 'gonzanama', 'macara', 'saraguro', 'sigsig', 'Loja', 'Catamayo', 'Macará', 'Cariamanga', 'Zapotillo', 'Celica', 'Chaguarpamba', 'Pindal', 'Alamor', 'Quilanga', 'Sozoranga', 'Calvas', 'Espíndola', 'Gonzanamá', 'Paltas', 'Puyango'],\n",
    "    'Los Ríos': ['urdaneta', 'Quevedo', 'Babahoyo', 'Ventanas', 'Buena Fe', 'Vinces', 'Valencia', 'Montalvo', 'Mocache', 'Palenque', 'Catarama', 'Quinsaloma', 'Puebloviejo', 'Baba'],\n",
    "    'Tungurahua': ['santiago de pillaro', 'Ambato', 'Baños de Agua Santa', 'Pelileo', 'Cevallos', 'Mocha', 'Patate', 'Quero', 'San Pedro de Pelileo', 'Santiago de Píllaro', 'Tisaleo'],\n",
    "    'Chimborazo': ['cumanda', 'alausi', 'Riobamba', 'Guamote', 'Chambo', 'Chunchi', 'Guano', 'Pallatanga', 'Penipe', 'Cumandá', 'Alausí', 'Colta'],\n",
    "    'Esmeraldas': ['quininde', 'Esmeraldas', 'Rosa Zárate', 'San Lorenzo', 'Atacames', 'Muisne', 'Rioverde', 'Eloy Alfaro', 'Quinindé'],\n",
    "    'Cotopaxi': ['la mana', 'pujili', 'saquisili', 'Latacunga', 'La Maná', 'Pujilí', 'Sigchos', 'Saquisilí', 'Pangua', 'Salcedo'],\n",
    "    'Imbabura': ['san miguel de urcuqui', 'Ibarra', 'Otavalo', 'Cotacachi', 'Pimampiro', 'Urcuquí', 'Atuntaqui', 'Antonio Ante', 'San Miguel de Urcuquí'],\n",
    "    'Santa Elena': ['La Libertad', 'Santa Elena', 'Salinas', 'Salinas (Salinas, Cab. Cantonal)'],\n",
    "    'Carchi': ['bolivar', 'montufar', 'tulcan', 'Tulcán', 'San Gabriel', 'Huaca', 'Bolívar', 'Mira', 'Espejo', 'Montúfar', 'San Pedro de Huaca'],\n",
    "    'Sucumbíos': ['sucumbios', 'Nueva Loja', 'Shushufindi', 'Cascales', 'Cuyabeno', 'Gonzalo Pizarro', 'Putumayo', 'Sucumbíos', 'Lago Agrio'],\n",
    "    'Pastaza': ['pastaza', 'Puyo', 'Mera', 'Santa Clara', 'Arajuno'],\n",
    "    'Orellana': ['El Coca', 'La Joya de los Sachas', 'Aguarico', 'Loreto', 'Puerto Francisco de Orellana'],\n",
    "    'Morona Santiago': ['limon indanza', 'sucua', 'Macas', 'Gualaquiza', 'Morona', 'Palora', 'Sucúa', 'Logroño', 'Santiago', 'Taisha', 'Huamboya', 'San Juan Bosco', 'Limón Indanza', 'Pablo Sexto', 'Tiwintza'],\n",
    "    'Zamora Chinchipe': ['centinela del condor', 'Zamora', 'Yantzaza', 'El Pangui', 'Yacuambi', 'Centinela del Cóndor', 'Nangaritza', 'Paquisha', 'Chinchipe', 'Palanda'],\n",
    "    'Cañar': ['biblian', 'deleg', 'nabon', 'pucara', 'Azogues', 'La Troncal', 'Biblián', 'Déleg', 'Suscal', 'Biblían', 'Cañar', 'El Tambo'],\n",
    "    'Napo': ['Tena', 'Archidona', 'Carlos Julio Arosemena Tola', 'El Chaco', 'Quijos'],\n",
    "    'Bolívar': ['echeandia', 'Guaranda', 'Chillanes', 'Echeandía', 'San Miguel', 'Chimbo', 'Caluma', 'Echeandía'],\n",
    "    'Galápagos': ['san cristobal', 'Puerto Baquerizo Moreno', 'Puerto Ayora', 'Puerto Villamil', 'Isabela', 'San Cristóbal', 'Santa Cruz'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-19 02:47:33.553 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\jeanf\\miniconda3\\envs\\streamlit\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'last_clicked': None,\n",
       " 'last_object_clicked': None,\n",
       " 'last_object_clicked_tooltip': None,\n",
       " 'last_object_clicked_popup': None,\n",
       " 'all_drawings': None,\n",
       " 'last_active_drawing': None,\n",
       " 'bounds': {'_southWest': {'lat': -5.015, 'lng': -92.008},\n",
       "  '_northEast': {'lat': 1.681, 'lng': -75.193}},\n",
       " 'zoom': 6,\n",
       " 'last_circle_radius': None,\n",
       " 'last_circle_polygon': None}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from streamlit_folium import st_folium\n",
    "\n",
    "\n",
    "\n",
    "# Convertir el diccionario a un DataFrame\n",
    "data = {'Provincia': list(provincias_y_ciudades.keys()), \n",
    "        'Valor': [len(ciudades) for ciudades in provincias_y_ciudades.values()]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Cargar el archivo GeoJSON\n",
    "geojson_file = 'provinces.geojson'\n",
    "gdf = gpd.read_file(geojson_file)\n",
    "\n",
    "# Crear un mapa base centrado en Ecuador\n",
    "m = folium.Map(location=[-1.8312, -78.1834], zoom_start=6)\n",
    "\n",
    "# Agregar el choropleth map\n",
    "folium.Choropleth(\n",
    "    geo_data=gdf,\n",
    "    name='choropleth',\n",
    "    data=df,\n",
    "    columns=['Provincia', 'Valor'],\n",
    "    key_on='feature.properties.province',  # Ajustado según el campo correcto en tu GeoJSON\n",
    "    fill_color='YlGnBu',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    legend_name='Cantidad de Ciudades por Provincia'\n",
    ").add_to(m)\n",
    "\n",
    "# Mostrar el mapa en Streamlit\n",
    "st.title('Choropleth Map de Ecuador')\n",
    "st.write('Este es un ejemplo de un Choropleth Map que muestra la cantidad de ciudades por provincia en Ecuador.')\n",
    "\n",
    "# Renderizar el mapa en la aplicación Streamlit\n",
    "st_folium(m, width=700, height=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   country    province iso_1 iso_2  \\\n",
      "0  Ecuador       Azuay    EC  EC-A   \n",
      "1  Ecuador     Bolívar    EC  EC-B   \n",
      "2  Ecuador      Carchi    EC  EC-C   \n",
      "3  Ecuador       Cañar    EC  EC-F   \n",
      "4  Ecuador  Chimborazo    EC  EC-H   \n",
      "\n",
      "                                            geometry  \n",
      "0  POLYGON ((-79.76400 -3.06200, -79.76100 -3.063...  \n",
      "1  POLYGON ((-79.28700 -1.20700, -79.28100 -1.207...  \n",
      "2  POLYGON ((-78.51000 1.18900, -78.50800 1.19200...  \n",
      "3  POLYGON ((-79.35900 -2.36300, -79.35900 -2.367...  \n",
      "4  POLYGON ((-79.12900 -2.20300, -79.12800 -2.203...  \n"
     ]
    }
   ],
   "source": [
    "gdf = gpd.read_file(geojson_file)\n",
    "print(gdf.head())  # Mostrar las primeras filas para inspeccionar las propiedades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    " \n",
    "# Create a datetime slider with a range of one week\n",
    "start_date = datetime(2020, 1, 1)\n",
    "end_date = start_date + timedelta(weeks=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(start_date) # Mostrar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "influxdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
